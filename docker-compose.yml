services:
  embed:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8080:8080"
    command: >
      --hf-repo Qwen/Qwen3-Embedding-0.6B-GGUF
      --hf-file qwen3-embedding-0.6b-q8_0.gguf
      --embedding --pooling last
      -ub 8192
      --host 0.0.0.0 --port 8080
      --no-webui
    volumes:
      - hf-cache:/root/.cache/huggingface

  rerank:
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8181:8181"
    command: >
      --hf-repo ggml-org/Qwen3-Reranker-0.6B-Q8_0-GGUF
      --hf-file qwen3-reranker-0.6b-q8_0.gguf
      --rerank
      -c 4096
      --host 0.0.0.0 --port 8181
      --no-webui
    volumes:
      - hf-cache:/root/.cache/huggingface

volumes:
  hf-cache:
